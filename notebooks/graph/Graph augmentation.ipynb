{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d6e9b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0,'../../src')\n",
    "\n",
    "from hu_nmt.data_augmentator.dependency_parsers.spacy_dependency_parser import SpacyDependencyParser\n",
    "from hu_nmt.data_augmentator.dependency_parsers.stanza_dependency_parser import StanzaDependencyParser\n",
    "\n",
    "from hu_nmt.data_augmentator.wrapper.dependency_graph_wrapper import DependencyGraphWrapper\n",
    "from hu_nmt.data_augmentator.augmentators.subject_object_augmentator import SubjectObjectAugmentator\n",
    "\n",
    "from hu_nmt.data_augmentator.graph_mappers.ged import GED\n",
    "from hu_nmt.data_augmentator.graph_mappers.edge_mapper import EdgeMapper\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from networkx import optimize_graph_edit_distance, graph_edit_distance,optimize_edit_paths\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2998ec53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f6b055233e4a0da225a598f8c6ea60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 12:02:11 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2022-06-02 12:02:12 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2022-06-02 12:02:12 INFO: Use device: cpu\n",
      "2022-06-02 12:02:12 INFO: Loading: tokenize\n",
      "2022-06-02 12:02:12 INFO: Loading: pos\n",
      "2022-06-02 12:02:12 INFO: Loading: lemma\n",
      "2022-06-02 12:02:12 INFO: Loading: depparse\n",
      "2022-06-02 12:02:12 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "hun_dep_parser = SpacyDependencyParser(lang='hu')\n",
    "eng_dep_parser = StanzaDependencyParser('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b452de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ged = GED('hu', 'en')\n",
    "edge_mapper = EdgeMapper('hu', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c2a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hu_sent_file = 'data/cands/cands.hu'\n",
    "en_sent_file = 'data/cands/cands.en'\n",
    "\n",
    "\n",
    "with open(hu_sent_file, 'r') as f:\n",
    "    hu_sents = f.readlines()\n",
    "    hu_sents = [s.rstrip() for s in hu_sents]\n",
    "with open(en_sent_file, 'r') as f:\n",
    "    en_sents = f.readlines()\n",
    "    en_sents = [s.rstrip() for s in en_sents]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "15a3088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_augmentable(hu_graph1, en_graph1, hu_graph2, en_graph2):\n",
    "    if SubjectObjectAugmentator.is_eligible_for_augmentation(hu_graph1, en_graph1, 'nsubj') and \\\n",
    "        SubjectObjectAugmentator.is_eligible_for_augmentation(hu_graph2, en_graph2, 'nsubj'):\n",
    "        return 'nsubj'\n",
    "    if SubjectObjectAugmentator.is_eligible_for_augmentation(hu_graph1, en_graph1, 'obj') and \\\n",
    "        SubjectObjectAugmentator.is_eligible_for_augmentation(hu_graph2, en_graph2, 'obj'):\n",
    "        return 'obj'\n",
    "    return None\n",
    "\n",
    "def try_augmentation():\n",
    "    while True:\n",
    "        (idx1, idx2) = np.random.choice(len(hu_sents), 2, replace=False)\n",
    "\n",
    "        # Graphs\n",
    "\n",
    "        hu_sent1 = hu_sents[idx1]\n",
    "        en_sent1 = en_sents[idx1]\n",
    "\n",
    "        hu_sent2 = hu_sents[idx2]\n",
    "        en_sent2 = en_sents[idx2]\n",
    "\n",
    "        hu_graph1 = hun_dep_parser.sentence_to_graph_wrapper(hu_sent1)\n",
    "        en_graph1 = eng_dep_parser.sentence_to_graph_wrapper(en_sent1)\n",
    "\n",
    "        hu_graph2 = hun_dep_parser.sentence_to_graph_wrapper(hu_sent2)\n",
    "        en_graph2 = eng_dep_parser.sentence_to_graph_wrapper(en_sent2)\n",
    "\n",
    "        dep = check_if_augmentable(hu_graph1, en_graph1, hu_graph2, en_graph2)\n",
    "\n",
    "        if dep is not None:\n",
    "            break\n",
    "    \n",
    "    # hu_graph1.display_graph()\n",
    "    # en_graph1.display_graph()\n",
    "    # hu_graph2.display_graph()\n",
    "    # en_graph2.display_graph()\n",
    "    \n",
    "\n",
    "    # Similarity\n",
    "    \n",
    "    hu_dist = ged.get_normalized_distance(hu_graph1.graph, hu_graph2.graph)\n",
    "    en_dist = ged.get_normalized_distance(en_graph1.graph, en_graph2.graph)\n",
    "    \n",
    "    hu_jaccard = edge_mapper.get_jaccard_index(hu_graph1.graph, hu_graph2.graph)\n",
    "    en_jaccard = edge_mapper.get_jaccard_index(en_graph1.graph, en_graph2.graph)\n",
    "    \n",
    "    # Augmentation\n",
    "    \n",
    "    augmentator = SubjectObjectAugmentator(None, None, 0, 0, [], '', '')\n",
    "    \n",
    "    new_hu_sents = augmentator.swap_subtrees(hu_graph1, hu_graph2, dep)\n",
    "    new_en_sents = augmentator.swap_subtrees(en_graph1, en_graph2, dep)\n",
    "    \n",
    "    \n",
    "    # Check augmentation\n",
    "    \n",
    "    aug_hu_graph1 = hun_dep_parser.sentence_to_graph_wrapper(new_hu_sents[0])\n",
    "    aug_en_graph1 = eng_dep_parser.sentence_to_graph_wrapper(new_en_sents[0])\n",
    "\n",
    "    aug_hu_graph2 = hun_dep_parser.sentence_to_graph_wrapper(new_hu_sents[1])\n",
    "    aug_en_graph2 = eng_dep_parser.sentence_to_graph_wrapper(new_en_sents[1])\n",
    "    \n",
    "    aug_hu1_dist = ged.get_normalized_distance(hu_graph1.graph, aug_hu_graph1.graph)\n",
    "    aug_en1_dist = ged.get_normalized_distance(en_graph1.graph, aug_en_graph1.graph)\n",
    "    aug_hu2_dist = ged.get_normalized_distance(hu_graph2.graph, aug_hu_graph2.graph)\n",
    "    aug_en2_dist = ged.get_normalized_distance(en_graph2.graph, aug_en_graph2.graph)\n",
    "    \n",
    "    aug_hu1_jaccard = edge_mapper.get_jaccard_index(hu_graph1.graph, aug_hu_graph1.graph)\n",
    "    aug_en1_jaccard = edge_mapper.get_jaccard_index(en_graph1.graph, aug_en_graph1.graph)\n",
    "    aug_hu2_jaccard = edge_mapper.get_jaccard_index(hu_graph2.graph, aug_hu_graph2.graph)\n",
    "    aug_en2_jaccard = edge_mapper.get_jaccard_index(en_graph2.graph, aug_en_graph2.graph)\n",
    "    \n",
    "    # Printing\n",
    "    print('\\n-----------Original-----------\\n')\n",
    "    print(f'Hu-1: {hu_sent1}\\nEn-1{en_sent1}\\n')\n",
    "    print(f'Hu-2: {hu_sent2}\\nEn-2{en_sent2}')\n",
    "    \n",
    "    print('\\n-----------Similarity-----------\\n')\n",
    "    print(f'Hu norm ged: {hu_dist}')\n",
    "    print(f'En norm ged: {en_dist}\\n')\n",
    "    \n",
    "    print(f'Hu jaccard: {hu_jaccard}')\n",
    "    print(f'En jaccard: {en_jaccard}\\n')\n",
    "    \n",
    "    print('\\n-----------Augmentation-----------\\n')\n",
    "    print(f'{new_hu_sents[0]}\\n{new_en_sents[0]}\\n')\n",
    "    print(f'{new_hu_sents[1]}\\n{new_en_sents[1]}')\n",
    "    \n",
    "    print('\\n-----------Check Augmentation-----------\\n')\n",
    "    print(f'Hu-1\\n\\tnorm ged: {aug_hu1_dist}\\n\\tjaccard: {aug_hu1_jaccard}')\n",
    "    print(f'Hu-2\\n\\tnorm ged: {aug_hu2_dist}\\n\\tjaccard: {aug_hu2_jaccard}')\n",
    "    print(f'En-1\\n\\tnorm ged: {aug_en1_dist}\\n\\tjaccard: {aug_en1_jaccard}')\n",
    "    print(f'En-2\\n\\tnorm ged: {aug_en2_dist}\\n\\tjaccard: {aug_en2_jaccard}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "77651c14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAAAAAAAAAAAAAAAAA\n",
      "AAAAAAAAAAAAAAAAAA\n",
      "AAAAAAAAAAAAAAAAAA\n",
      "AAAAAAAAAAAAAAAAAA\n",
      "AAAAAAAAAAAAAAAAAA\n",
      "AAAAAAAAAAAAAAAAAA\n",
      "AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "-----------Original-----------\n",
      "\n",
      "Hu-1: Az a haderő, amelynek nincsen külső célpontja, mindig a saját népe ellen fordul.\n",
      "En-1The military force which is denied an external target always turns against its own people.\n",
      "\n",
      "Hu-2: Az ifjú elnyújtózott a padkán egy vadalmafa alatt.\n",
      "En-2The youth lay down on the bench, under a wild apple-tree.\n",
      "\n",
      "-----------Similarity-----------\n",
      "\n",
      "Hu norm ged: 0.6\n",
      "En norm ged: 0.6129032258064516\n",
      "\n",
      "Hu jaccard: 0.47058823529411764\n",
      "En jaccard: 0.34782608695652173\n",
      "\n",
      "\n",
      "-----------Augmentation-----------\n",
      "\n",
      "Az a haderő , amelynek nincsen Az ifjú , mindig a saját népe ellen fordul .\n",
      "the youth always turns against its own people .\n",
      "\n",
      "külső célpontja elnyújtózott a padkán egy vadalmafa alatt .\n",
      "the military force which is denied an external target lay down on the bench , under a wild apple - tree .\n",
      "\n",
      "-----------Check Augmentation-----------\n",
      "\n",
      "Hu-1\n",
      "\tnorm ged: 0.9375\n",
      "\tjaccard: 0.8823529411764706\n",
      "Hu-2\n",
      "\tnorm ged: 0.8333333333333334\n",
      "\tjaccard: 0.6363636363636364\n",
      "En-1\n",
      "\tnorm ged: 0.72\n",
      "\tjaccard: 0.5625\n",
      "En-2\n",
      "\tnorm ged: 0.8108108108108109\n",
      "\tjaccard: 0.6818181818181818\n"
     ]
    }
   ],
   "source": [
    "try_augmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67f32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa43de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc11e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9acb004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
