{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "drawn-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir, mkdir\n",
    "from os.path import isfile, join, exists, getsize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "previous-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 500\n",
    "hunglish_path = '../data/ftp.mokk.bme.hu/Hunglish2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "welcome-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_path = join(hunglish_path, 'combined')\n",
    "if not exists(combined_path):\n",
    "    mkdir(combined_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "annoying-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HunglishSampler:\n",
    "    def __init__(self, base_data_dir, sample_from_domains, seed, samples_per_domain_in_valid, samples_per_domain_in_test):\n",
    "        self.domains = [\n",
    "            'classic.lit',\n",
    "            'law',\n",
    "            'modern.lit',\n",
    "            'softwaredocs',\n",
    "            'subtitles'\n",
    "        ]\n",
    "        self.base_data_dir = base_data_dir\n",
    "        self.sample_from_domains = sample_from_domains\n",
    "        self.RANDOM_SEED = seed\n",
    "        random.seed(self.RANDOM_SEED)\n",
    "        self.train_set = pd.DataFrame()\n",
    "        self.validation_set = pd.DataFrame()\n",
    "        self.test_set = pd.DataFrame()\n",
    "        self.CORPUS_LENGTH = 2974471 # Need to know in advance to calculate train/valid/test ratios\n",
    "        self.valid_set_ratio = samples_per_domain_in_valid * len(self.domains) / self.CORPUS_LENGTH\n",
    "        self.test_set_ratio = samples_per_domain_in_test * len(self.domains) / self.CORPUS_LENGTH\n",
    "        self.train_set_ratio = 1 - self.valid_set_ratio - self.test_set_ratio\n",
    "        \n",
    "        print(f'Valid set ratio: {self.valid_set_ratio}')\n",
    "        print(f'Test set ratio: {self.test_set_ratio}')\n",
    "        print(f'Train set ratio: {self.train_set_ratio}')\n",
    "        \n",
    "        \n",
    "    def sample(self, max_number_of_tokens=-1, sample_ratio=1.0):\n",
    "        data = {\n",
    "            'train': {\n",
    "                'hun': [],\n",
    "                'eng': [],\n",
    "                'source_file': [],\n",
    "                'domain': []\n",
    "            },\n",
    "            'valid': {\n",
    "                'hun': [],\n",
    "                'eng': [],\n",
    "                'source_file': [],\n",
    "                'domain': []\n",
    "            },\n",
    "            'test': {\n",
    "                'hun': [],\n",
    "                'eng': [],\n",
    "                'source_file': [],\n",
    "                'domain': []\n",
    "                \n",
    "            }\n",
    "        }\n",
    "        sentences_dropped = 0\n",
    "        for domain in self.sample_from_domains:\n",
    "            if domain not in self.domains:\n",
    "                raise ValueError(f'Cannot sample from domain {domain}')\n",
    "            domain_path = f'{self.base_data_dir}/{domain}/bi'\n",
    "            files = [f for f in listdir(f'{domain_path}') if isfile(join(f'{domain_path}', f))]\n",
    "            for file in files:\n",
    "                file_path = f'{domain_path}/{file}'\n",
    "                with open(file_path, 'r', encoding='latin2') as f:\n",
    "                    # Train-test split file-wise\n",
    "                    hun_sentences = []\n",
    "                    eng_sentences = []\n",
    "                    malformed_lines = {}\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            hun_sentence, eng_sentence = line.rstrip('\\n').split('\\t')\n",
    "                            if max_number_of_tokens == -1 or (max_number_of_tokens != -1 and len(hun_sentence.split()) < max_number_of_tokens and len(eng_sentence.split()) < max_number_of_tokens):\n",
    "                                hun_sentences.append(hun_sentence)\n",
    "                                eng_sentences.append(eng_sentence)\n",
    "                            else:\n",
    "                                sentences_dropped += 1\n",
    "                            if len(hun_sentences) != eng_sentences:\n",
    "                                raise ValueError(f'Hun-eng sentence pair has bad formatting')\n",
    "                        except:\n",
    "                            if domain not in malformed_lines:\n",
    "                                malformed_lines[domain] = []\n",
    "                            malformed_lines[domain].append((f'line: {line}', f'file: {file}'))\n",
    "                            \n",
    "                    if sample_ratio != 1.0:\n",
    "                        n_sentences = len(hun_sentences)\n",
    "                        ids = list(range(n_sentences))\n",
    "                        sampled_ids = random.sample(ids, int(n_sentences * sample_ratio))\n",
    "                        hun_sentences = [hun_sentences[i] for i in sampled_ids]\n",
    "                        eng_sentences = [eng_sentences[i] for i in sampled_ids]\n",
    "                    \n",
    "                    try:\n",
    "                        x_train, x_test, y_train, y_test = train_test_split(hun_sentences, eng_sentences,\n",
    "                                                                           train_size=self.train_set_ratio,\n",
    "                                                                           test_size=self.test_set_ratio,\n",
    "                                                                           random_state=self.RANDOM_SEED)\n",
    "\n",
    "                        x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train,\n",
    "                                                                              train_size=1-self.valid_set_ratio,\n",
    "                                                                              test_size=self.valid_set_ratio,\n",
    "                                                                              random_state=self.RANDOM_SEED)\n",
    "                    except Exception as e:\n",
    "                        if 'the resulting train set will be empty' not in str(e):\n",
    "                            print(e)\n",
    "                            print(f'There might not be well formatted lines in file: {file}, size: {getsize(file_path)} bytes') # look into these files\n",
    "                        \n",
    "                    data['train']['hun'].extend(x_train)\n",
    "                    data['train']['eng'].extend(y_train)\n",
    "                    data['train']['source_file'].extend([file for _ in range(len(x_train))])\n",
    "                    data['train']['domain'].extend([domain for _ in range(len(x_train))])\n",
    "\n",
    "                    data['valid']['hun'].extend(x_valid)\n",
    "                    data['valid']['eng'].extend(y_valid)\n",
    "                    data['valid']['source_file'].extend([file for _ in range(len(x_valid))])\n",
    "                    data['valid']['domain'].extend([domain for _ in range(len(x_valid))])\n",
    "\n",
    "                    data['test']['domain'].extend([domain for _ in range(len(x_test))])                    \n",
    "                    data['test']['hun'].extend(x_test)\n",
    "                    data['test']['eng'].extend(y_test)\n",
    "                    data['test']['source_file'].extend([file for _ in range(len(x_test))])\n",
    "        \n",
    "        if max_number_of_tokens != -1:\n",
    "            print('Sentences dropped: ', sentences_dropped)\n",
    "        print('Train set length: {}'.format(len(data['train']['hun'])))\n",
    "        print('Validation set length: {}'.format(len(data['valid']['hun'])))\n",
    "        print('Test set length: {}'.format(len(data['test']['hun'])))\n",
    "        print('--------TRAIN--------')\n",
    "        print(data['train']['hun'][0:3])\n",
    "        print(data['train']['eng'][0:3])\n",
    "        print('--------VALID--------')\n",
    "        print(data['valid']['hun'][0:3])\n",
    "        print(data['valid']['eng'][0:3])\n",
    "        print('--------TEST--------')\n",
    "        print(data['test']['hun'][0:3])\n",
    "        print(data['test']['eng'][0:3])\n",
    "\n",
    "        # Dump splits to dataframes\n",
    "        self.train_set = pd.DataFrame(data['train'])\n",
    "        self.validation_set = pd.DataFrame(data['valid'])\n",
    "        self.test_set = pd.DataFrame(data['test'])\n",
    "        \n",
    "    def save_splits_to_csv(self):\n",
    "        self.train_set.to_csv('./train_set.csv')\n",
    "        self.validation_set.to_csv('./validation_set.csv')\n",
    "        self.test_set.to_csv('./test_set.csv')\n",
    "        \n",
    "    def create_data_set_files(self, path, base_file_name):\n",
    "        file_name_beginning = join(path, base_file_name + '-')\n",
    "        f = lambda set_name, language: file_name_beginning + set_name + '.' + language\n",
    "\n",
    "        self.train_set['hun'].to_csv(f('train', 'hu'), encoding='utf-8', header=None, index=None, sep=' ')\n",
    "        self.train_set['eng'].to_csv(f('train', 'en'), encoding='utf-8', header=None, index=None, sep=' ')\n",
    "        self.validation_set['hun'].to_csv(f('valid', 'hu'), encoding='utf-8', header=None, index=None, sep=' ')\n",
    "        self.validation_set['eng'].to_csv(f('valid', 'en'), encoding='utf-8', header=None, index=None, sep=' ')\n",
    "        self.test_set['hun'].to_csv(f('test', 'hu'), encoding='utf-8', header=None, index=None, sep=' ')\n",
    "        self.test_set['eng'].to_csv(f('test', 'en'), encoding='utf-8', header=None, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "weighted-observer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid set ratio: 0.008404855855041115\n",
      "Test set ratio: 0.008404855855041115\n",
      "Train set ratio: 0.9831902882899177\n"
     ]
    }
   ],
   "source": [
    "sampler = HunglishSampler(\n",
    "                base_data_dir=hunglish_path,\n",
    "                sample_from_domains= [\n",
    "                        'classic.lit',\n",
    "                        'law',\n",
    "                        'modern.lit',\n",
    "                        'softwaredocs',\n",
    "                        'subtitles'\n",
    "                        ],\n",
    "                samples_per_domain_in_valid=5000,\n",
    "                samples_per_domain_in_test=5000,\n",
    "                seed=RANDOM_SEED\n",
    "                \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "facial-southwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences dropped:  3\n",
      "Train set length: 2738591\n",
      "Validation set length: 23259\n",
      "Test set length: 23660\n",
      "--------TRAIN--------\n",
      "['Te, akit a föld határairól hoztalak elő, és a világ végéről hívtalak meg; te, akihez így szóltam: Szolgám vagy, kiválasztottalak, ezért nem vetlek el: Ne félj, mert veled vagyok, ne csüggedj, mert én vagyok a te Istened!', 'Ha majd végzel a pusztítással, elpusztítanak, és ha beteltél a fosztogatással, téged is kifosztanak.', 'Az álnok szívűektől iszonyodik az Úr, akik tisztességesek, elnyerik tetszését.']\n",
      "['In whom I have taken thee from the ends of the earth, and from the remote parts thereof have called thee, and said to thee: Thou art my servant, I have chosen thee, and have not cast thee away.', 'O Lord, have mercy on us: for we have waited for thee: be thou our arm in the morning, and our salvation in the time of trouble.', 'A perverse heart is abominable to the Lord: and his will is in them that walk sincerely.']\n",
      "--------VALID--------\n",
      "['Mert elküldi angyalait hozzád, hogy védelmezzenek minden utadon.', 'Nemcsak hogy utánoztad az útjaikat, és elkövetted gyalázatos tetteiket, hanem még romlottabb voltál, mint ők, egész életmódodban.', 'Nemde még a nap is megállt intésére, s egy napból kettő lett?']\n",
      "['For he hath given his angels charge over thee; to keep thee in all thy ways.', 'But neither hast thou walked in their ways, nor hast thou done a little less than they according to their wickednesses: thou hast done almost more wicked things than they in all thy ways.', 'Was not the sun stopped in his anger, and one day made as two?']\n",
      "--------TEST--------\n",
      "['Hozzá is láttam, hogy legeltessem a leölésre szánt nyájat a kereskedőknek.', 'Ha kinyitja a száját, bölcsen beszél, jóságos tanítás (árad) a nyelvéről.', 'Kinyilvánította az Úr üdvösségét, igazságosságát feltárta a pogányok előtt.']\n",
      "['And I will feed the flock of slaughter for this, O ye poor of the flock.', 'She hath opened her mouth to wisdom, and the law of clemency is on her tongue.', 'The Lord hath made known his salvation: he hath revealed his justice in the sight of the Gentiles.']\n"
     ]
    }
   ],
   "source": [
    "sampler.sample(max_number_of_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "numerical-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.create_data_set_files(hunglish_path, 'combined/hunglish2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "entertaining-jordan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences dropped:  3\n",
      "Train set length: 2738591\n",
      "Validation set length: 23259\n",
      "Test set length: 23660\n",
      "--------TRAIN--------\n",
      "['Te, akit a föld határairól hoztalak elő, és a világ végéről hívtalak meg; te, akihez így szóltam: Szolgám vagy, kiválasztottalak, ezért nem vetlek el: Ne félj, mert veled vagyok, ne csüggedj, mert én vagyok a te Istened!', 'Ha majd végzel a pusztítással, elpusztítanak, és ha beteltél a fosztogatással, téged is kifosztanak.', 'Az álnok szívűektől iszonyodik az Úr, akik tisztességesek, elnyerik tetszését.']\n",
      "['In whom I have taken thee from the ends of the earth, and from the remote parts thereof have called thee, and said to thee: Thou art my servant, I have chosen thee, and have not cast thee away.', 'O Lord, have mercy on us: for we have waited for thee: be thou our arm in the morning, and our salvation in the time of trouble.', 'A perverse heart is abominable to the Lord: and his will is in them that walk sincerely.']\n",
      "--------VALID--------\n",
      "['Mert elküldi angyalait hozzád, hogy védelmezzenek minden utadon.', 'Nemcsak hogy utánoztad az útjaikat, és elkövetted gyalázatos tetteiket, hanem még romlottabb voltál, mint ők, egész életmódodban.', 'Nemde még a nap is megállt intésére, s egy napból kettő lett?']\n",
      "['For he hath given his angels charge over thee; to keep thee in all thy ways.', 'But neither hast thou walked in their ways, nor hast thou done a little less than they according to their wickednesses: thou hast done almost more wicked things than they in all thy ways.', 'Was not the sun stopped in his anger, and one day made as two?']\n",
      "--------TEST--------\n",
      "['Hozzá is láttam, hogy legeltessem a leölésre szánt nyájat a kereskedőknek.', 'Ha kinyitja a száját, bölcsen beszél, jóságos tanítás (árad) a nyelvéről.', 'Kinyilvánította az Úr üdvösségét, igazságosságát feltárta a pogányok előtt.']\n",
      "['And I will feed the flock of slaughter for this, O ye poor of the flock.', 'She hath opened her mouth to wisdom, and the law of clemency is on her tongue.', 'The Lord hath made known his salvation: he hath revealed his justice in the sight of the Gentiles.']\n"
     ]
    }
   ],
   "source": [
    "sampler.sample(max_number_of_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "renewable-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.create_data_set_files(hunglish_path, 'combined/hunglish2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-treatment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
