{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "drawn-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir, mkdir\n",
    "from os.path import isfile, join, exists, getsize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "previous-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 500\n",
    "hunglish_path = '../data/ftp.mokk.bme.hu/Hunglish2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "welcome-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_path = join(hunglish_path, 'combined')\n",
    "if not exists(combined_path):\n",
    "    mkdir(combined_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "annoying-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HunglishSampler:\n",
    "    def __init__(self, base_data_dir, sample_from_domains, seed, samples_per_domain_in_valid, samples_per_domain_in_test):\n",
    "        self.domains = [\n",
    "            'classic.lit',\n",
    "            'law',\n",
    "            'modern.lit',\n",
    "            'softwaredocs',\n",
    "            'subtitles'\n",
    "        ]\n",
    "        self.base_data_dir = base_data_dir\n",
    "        self.sample_from_domains = sample_from_domains\n",
    "        self.RANDOM_SEED = seed\n",
    "        random.seed(self.RANDOM_SEED)\n",
    "        self.train_set = pd.DataFrame()\n",
    "        self.validation_set = pd.DataFrame()\n",
    "        self.test_set = pd.DataFrame()\n",
    "        self.CORPUS_LENGTH = 2974471 # Need to know in advance to calculate train/valid/test ratios\n",
    "        self.valid_set_ratio = samples_per_domain_in_valid * len(self.domains) / self.CORPUS_LENGTH\n",
    "        self.test_set_ratio = samples_per_domain_in_test * len(self.domains) / self.CORPUS_LENGTH\n",
    "        self.train_set_ratio = 1 - self.valid_set_ratio - self.test_set_ratio\n",
    "        \n",
    "        print(f'Valid set ratio: {self.valid_set_ratio}')\n",
    "        print(f'Test set ratio: {self.test_set_ratio}')\n",
    "        print(f'Train set ratio: {self.train_set_ratio}')\n",
    "        \n",
    "        \n",
    "    def sample(self, max_number_of_tokens=-1, sample_ratio=1.0):\n",
    "        data = {\n",
    "            'train': {\n",
    "                'hun': [],\n",
    "                'eng': [],\n",
    "                'source_file': [],\n",
    "                'domain': []\n",
    "            },\n",
    "            'valid': {\n",
    "                'hun': [],\n",
    "                'eng': [],\n",
    "                'source_file': [],\n",
    "                'domain': []\n",
    "            },\n",
    "            'test': {\n",
    "                'hun': [],\n",
    "                'eng': [],\n",
    "                'source_file': [],\n",
    "                'domain': []\n",
    "                \n",
    "            }\n",
    "        }\n",
    "        sentences_dropped = 0\n",
    "        for domain in self.sample_from_domains:\n",
    "            if domain not in self.domains:\n",
    "                raise ValueError(f'Cannot sample from domain {domain}')\n",
    "            domain_path = f'{self.base_data_dir}/{domain}/bi'\n",
    "            files = [f for f in listdir(f'{domain_path}') if isfile(join(f'{domain_path}', f))]\n",
    "            for file in files:\n",
    "                file_path = f'{domain_path}/{file}'\n",
    "                with open(file_path, 'r', encoding='latin2') as f:\n",
    "                    # Train-test split file-wise\n",
    "                    hun_sentences = []\n",
    "                    eng_sentences = []\n",
    "                    malformed_lines = {}\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            hun_sentence, eng_sentence = line.rstrip('\\n').split('\\t')\n",
    "                            if max_number_of_tokens == -1 or (max_number_of_tokens != -1 and len(hun_sentence.split()) < max_number_of_tokens and len(eng_sentence.split()) < max_number_of_tokens):\n",
    "                                hun_sentences.append(hun_sentence)\n",
    "                                eng_sentences.append(eng_sentence)\n",
    "                            else:\n",
    "                                sentences_dropped += 1\n",
    "                            if len(hun_sentences) != eng_sentences:\n",
    "                                raise ValueError(f'Hun-eng sentence pair has bad formatting')\n",
    "                        except:\n",
    "                            if domain not in malformed_lines:\n",
    "                                malformed_lines[domain] = []\n",
    "                            malformed_lines[domain].append((f'line: {line}', f'file: {file}'))\n",
    "                            \n",
    "                    if sample_ratio != 1.0:\n",
    "                        n_sentences = len(hun_sentences)\n",
    "                        ids = list(range(n_sentences))\n",
    "                        sampled_ids = random.sample(ids, int(n_sentences * sample_ratio))\n",
    "                        hun_sentences = [hun_sentences[i] for i in sampled_ids]\n",
    "                        eng_sentences = [eng_sentences[i] for i in sampled_ids]\n",
    "                    \n",
    "                    try:\n",
    "                        x_train, x_test, y_train, y_test = train_test_split(hun_sentences, eng_sentences,\n",
    "                                                                           train_size=self.train_set_ratio,\n",
    "                                                                           test_size=self.test_set_ratio,\n",
    "                                                                           random_state=self.RANDOM_SEED)\n",
    "\n",
    "                        x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train,\n",
    "                                                                              train_size=1-self.valid_set_ratio,\n",
    "                                                                              test_size=self.valid_set_ratio,\n",
    "                                                                              random_state=self.RANDOM_SEED)\n",
    "                    except Exception as e:\n",
    "                        if 'the resulting train set will be empty' not in str(e):\n",
    "                            print(e)\n",
    "                            print(f'There might not be well formatted lines in file: {file}, size: {getsize(file_path)} bytes') # look into these files\n",
    "                        \n",
    "                    data['train']['hun'].extend(x_train)\n",
    "                    data['train']['eng'].extend(y_train)\n",
    "                    data['train']['source_file'].extend([file for _ in range(len(x_train))])\n",
    "                    data['train']['domain'].extend([domain for _ in range(len(x_train))])\n",
    "\n",
    "                    data['valid']['hun'].extend(x_valid)\n",
    "                    data['valid']['eng'].extend(y_valid)\n",
    "                    data['valid']['source_file'].extend([file for _ in range(len(x_valid))])\n",
    "                    data['valid']['domain'].extend([domain for _ in range(len(x_valid))])\n",
    "\n",
    "                    data['test']['domain'].extend([domain for _ in range(len(x_test))])                    \n",
    "                    data['test']['hun'].extend(x_test)\n",
    "                    data['test']['eng'].extend(y_test)\n",
    "                    data['test']['source_file'].extend([file for _ in range(len(x_test))])\n",
    "        \n",
    "        if max_number_of_tokens != -1:\n",
    "            print('Sentences dropped: ', sentences_dropped)\n",
    "        print('Train set length: {}'.format(len(data['train']['hun'])))\n",
    "        print('Validation set length: {}'.format(len(data['valid']['hun'])))\n",
    "        print('Test set length: {}'.format(len(data['test']['hun'])))\n",
    "        print('--------TRAIN--------')\n",
    "        print(data['train']['hun'][0:3])\n",
    "        print(data['train']['eng'][0:3])\n",
    "        print('--------VALID--------')\n",
    "        print(data['valid']['hun'][0:3])\n",
    "        print(data['valid']['eng'][0:3])\n",
    "        print('--------TEST--------')\n",
    "        print(data['test']['hun'][0:3])\n",
    "        print(data['test']['eng'][0:3])\n",
    "\n",
    "        # Dump splits to dataframes\n",
    "        self.train_set = pd.DataFrame(data['train'])\n",
    "        self.validation_set = pd.DataFrame(data['valid'])\n",
    "        self.test_set = pd.DataFrame(data['test'])\n",
    "        \n",
    "    def save_splits_to_csv(self):\n",
    "        self.train_set.to_csv('./train_set.csv')\n",
    "        self.validation_set.to_csv('./validation_set.csv')\n",
    "        self.test_set.to_csv('./test_set.csv')\n",
    "        \n",
    "    def create_data_set_files(self, path, base_file_name):\n",
    "        file_name_beginning = join(path, base_file_name + '-')\n",
    "        f = lambda set_name, language: file_name_beginning + set_name + '.' + language\n",
    "\n",
    "        self.train_set['hun'].to_csv(f('train', 'hu'), header=None, index=None, sep=' ')\n",
    "        self.train_set['eng'].to_csv(f('train', 'en'), header=None, index=None, sep=' ')\n",
    "        self.validation_set['hun'].to_csv(f('valid', 'hu'), header=None, index=None, sep=' ')\n",
    "        self.validation_set['eng'].to_csv(f('valid', 'en'), header=None, index=None, sep=' ')\n",
    "        self.test_set['hun'].to_csv(f('test', 'hu'), header=None, index=None, sep=' ')\n",
    "        self.test_set['eng'].to_csv(f('test', 'en'), header=None, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "weighted-observer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid set ratio: 0.008404855855041115\n",
      "Test set ratio: 0.008404855855041115\n",
      "Train set ratio: 0.9831902882899177\n"
     ]
    }
   ],
   "source": [
    "sampler = HunglishSampler(\n",
    "                base_data_dir=hunglish_path,\n",
    "                sample_from_domains= [\n",
    "                        'classic.lit',\n",
    "                        'law',\n",
    "                        'modern.lit',\n",
    "                        'softwaredocs',\n",
    "                        'subtitles'\n",
    "                        ],\n",
    "                samples_per_domain_in_valid=5000,\n",
    "                samples_per_domain_in_test=5000,\n",
    "                seed=RANDOM_SEED\n",
    "                \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "facial-southwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences dropped:  130\n",
      "Train set length: 4051549\n",
      "Validation set length: 50240\n",
      "Test set length: 50800\n",
      "--------TRAIN--------\n",
      "['Büszke ez ének, minden szava, célja, Átfogni tér s idő roppant birodalmait, A fejlődést, a feltüremlőt, a növekvőt és a nemzedékeket.', 'Elmegyek, mint a levegő, fehér fürtjeimet rázom a szökevény napra, Örvényekbe árasztom húsomat és rostos sávokban hömpölyögtetem.', 'Legfőbb érdememet megtagadom tőletek, nem vetem le magamról igazi lényem, Világokat mérjetek meg, de sohase próbáljatok megmérni engem, A legügyesebbet, legjobbat is megdöbbentem, közületek, ha csak rátok nézek.']\n",
      "['Haughty this song, its words and scope, To span vast realms of space and time, Evolution - the cumulative - growths and generations.', 'I depart as air, I shake my white locks at the runaway sun, I effuse my flesh in eddies, and drift it in lacy jags.', 'My final merit I refuse you, I refuse putting from me what I really am, Encompass worlds, but never try to encompass me, I crowd your sleekest and best by simply looking toward you.']\n",
      "--------VALID--------\n",
      "['Gyöngéden bánok veled, kunkorodó fű, Lehet, hogy ifjak mellén fakadsz te, Lehet, hogy ismerve szerettem volna őket, Lehet, hogy aggoktól származol, vagy az anyaölből korán kivett csecsemőktől, És íme te vagy itt az anyaöl.', 'Nézz arcomba, míg én az esti levegőt szippantom, (Beszélj becsülettel, senki más nem hall és én csak egy pillanattal maradok tovább.)', 'Bilincseim és homokzsákjaim elhagynak, könyököm tengeröblökön pihen, Hegységeken csatangolok; tenyerem kontinenseket fed, Talpon vagyok látomásommal.']\n",
      "[\"Tenderly will I use you curling grass, It may be you transpire from the breasts of young men, It may be if I had known them I would have loved them, It may be you are from old people, or from offspring taken soon out of their mothers' laps, And here you are the mothers' laps.\", 'Look in my face while I snuff the sidle of evening, (Talk honestly, no one else hears you, and I stay only a minute longer.)', 'My ties and ballasts leave me, my elbows rest in sea-gaps, I skirt sierras, my palms cover continents, I am afoot with my vision.']\n",
      "--------TEST--------\n",
      "['', 'Én jól ismerem saját önzésemet, Habzsolásom útjait, kisebb dolgokról nem írhatok, Bárki légy, egyenlő vagy velem, s elragadlak én.', 'Rajtam hatol át sok régen elnémult hang, Véget nem érő rab- és rabszolganemzedékek hangjai, Betegek és elcsüggedtek és tolvajok és törpék hangjai, Felkészülődések és gyarapodások körforgásának hangjai, És a csillagokat egybekötő fonalak és anyaméhek és nemzősejtek hangjai, És azok jogainak hangjai, akiket mások letepertek, A torzalakúak, bohók, laposak, balgák, megvetettek hangjai, Légbeli köd, ganajtúró bogarakéi.']\n",
      "['', 'I know perfectly well my own egotism, Know my omnivorous lines and must not write any less, And would fetch you whoever you are flush with myself.', \"Through me many long dumb voices, Voices of the interminable generations of prisoners and slaves, Voices of the diseas'd and despairing and of thieves and dwarfs, Voices of cycles of preparation and accretion, And of the threads that connect the stars, and of wombs and of the father-stuff, And of the rights of them the others are down upon, Of the deform'd, trivial, flat, foolish, despised, Fog in the air, beetles rolling balls of dung.\"]\n"
     ]
    }
   ],
   "source": [
    "sampler.sample(max_number_of_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "numerical-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.create_data_set_files(hunglish_path, 'combined/hunglish2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "entertaining-jordan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences dropped:  130\n",
      "Train set length: 472662\n",
      "Validation set length: 23310\n",
      "Test set length: 23266\n",
      "--------TRAIN--------\n",
      "['A mozgóvilág csírái ártatlan játékukból hallgatva felkelnek, frissen felrügyeznek, Ferdén szökellnek a magasba s a mélybe.', \"Ott fenn van az ég - de nálad, vagy a szomszédnál vagy a túlsó oldalon szembe'?\", 'Gyilkos vagy féltékeny volt irántad az emberi nem, testvérem, nővérem?']\n",
      "['Hefts of the moving world at innocent gambols silently rising freshly exuding, Scooting obliquely high and low.', 'The sky up there - yet here or next door, or across the way?', 'Were mankind murderous or jealous upon you, my brother, my sister?']\n",
      "--------VALID--------\n",
      "['Jól sikerültek a fotók - de barátod s karodba zárt biztos feleséged?', '- Hozzá lehet adni mindenféle vacsorához ezen a napon? - kérdezte Scrooge.', 'City-beli üzletbarátai valóban csodálkoztak volna, ha hallják, amint Scrooge természetének egész komolyságával beszél ilyen dolgokról, igen különös, síró-nevető hangon, és ha látják fölhevült, izgatott arcát.']\n",
      "['The well-taken photographs - but your wife or friend close and solid in your arms?', \"`Would it apply to any kind of dinner on this day?' asked Scrooge.\", 'To hear Scrooge expending all the earnestness of his nature on such subjects, in a most extraordinary voice between laughing and crying; and to see his heightened and excited face would have been a surprise to his business friends in the city, indeed.']\n",
      "--------TEST--------\n",
      "['Az egyik pumpát kilőtték, azt hisszük süllyedünk is már.', 'Ugyanaz az arc: tökéletesen ugyanaz.', 'Egy szempillantás alatt székére kuporodott, s a tolla úgy vágtatott, mintha utol akarná érni az elmúlt kilenc órát.']\n",
      "['One of the pumps has been shot away, it is generally thought we are sinking.', 'The same face: the very same.', \"He was on his stool in a jiffy; driving away with his pen, as if he were trying to overtake nine o'clock.\"]\n"
     ]
    }
   ],
   "source": [
    "sampler.sample(max_number_of_tokens=512, sample_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "renewable-console",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((440300, 4), (23237, 4), (23195, 4))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.create_data_set_files(hunglish_path, 'combined/hunglish2small')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
