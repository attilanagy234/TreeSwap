{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "buried-proportion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RootLogger root (INFO)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from argparse import Namespace\n",
    "from collections import defaultdict, Counter\n",
    "import onmt\n",
    "from onmt.inputters.inputter import _load_vocab, _build_fields_vocab, get_fields, IterOnDevice\n",
    "from onmt.inputters.corpus import ParallelCorpus\n",
    "from onmt.inputters.dynamic_iterator import DynamicDatasetIter\n",
    "from onmt.translate import GNMTGlobalScorer, Translator, TranslationBuilder\n",
    "from onmt.utils.misc import set_random_seed\n",
    "\n",
    "from onmt.utils.logging import init_logger, logger\n",
    "init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "authorized-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "set_random_seed(1111, is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "earned-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_config = \"\"\"\n",
    "src_vocab_size: 128\n",
    "tgt_vocab_size: 128\n",
    "\n",
    "save_data: run/samples\n",
    "src_vocab: vocabs/vocab.en\n",
    "tgt_vocab: vocabs/vocab.hu\n",
    "\n",
    "# Corpus opts:\n",
    "data:\n",
    "    hunglish:\n",
    "        path_src: E://Data/hu-nmt/combined-en-hu/hunglish2-tiny-train.en\n",
    "        path_tgt: E://Data/hu-nmt/combined-en-hu/hunglish2-tiny-train.hu\n",
    "        transforms: [sentencepiece]\n",
    "        weight: 1\n",
    "    valid:\n",
    "        path_src: E://Data/hu-nmt/combined-en-hu/hunglish2-tiny-valid.en\n",
    "        path_tgt: E://Data/hu-nmt/combined-en-hu/hunglish2-tiny-valid.hu\n",
    "        transforms: [sentencepiece]\n",
    "\n",
    "#### Subword\n",
    "src_subword_model: sp_models/bpe_en.model\n",
    "tgt_subword_model: sp_models/bpe_hu.model\n",
    "src_subword_nbest: 1\n",
    "src_subword_alpha: 0.0\n",
    "tgt_subword_nbest: 1\n",
    "tgt_subword_alpha: 0.0\n",
    "\n",
    "src_seq_length: 16  # maximum source sequence length\n",
    "tgt_seq_length: 16  # maximum target sequence length\n",
    "\n",
    "# TRAIN\n",
    "\n",
    "# Train on a single GPU\n",
    "world_size: 1\n",
    "gpu_ranks: [0]\n",
    "\n",
    "# Batching\n",
    "batch_size: 96\n",
    "#queue_size: 16\n",
    "#accum_count: [3]\n",
    "\n",
    "# General opts\n",
    "save_model: run/model_no_qoutes\n",
    "keep_checkpoint: 10\n",
    "save_checkpoint_steps: 10000\n",
    "average_decay: 0.0005\n",
    "seed: 1234\n",
    "report_every: 100\n",
    "train_steps: 400000\n",
    "valid_steps: 10000 \n",
    "single_pass: False\n",
    "early_stopping: 5 \n",
    "early_stopping_criteria: ppl\n",
    "\n",
    "# Optimization\n",
    "model_dtype: \"fp16\"\n",
    "optim: \"adam\"\n",
    "learning_rate: 2.0\n",
    "warmup_steps: 8000\n",
    "decay_method: \"noam\"\n",
    "adam_beta2: 0.998\n",
    "max_grad_norm: 0\n",
    "label_smoothing: 0.1\n",
    "param_init: 0\n",
    "param_init_glorot: true\n",
    "normalization: \"tokens\"\n",
    "\n",
    "# Model\n",
    "encoder_type: transformer\n",
    "decoder_type: transformer\n",
    "enc_layers: 2\n",
    "dec_layers: 2\n",
    "heads: 8\n",
    "rnn_size: 512\n",
    "word_vec_size: 512\n",
    "transformer_ff: 2048\n",
    "dropout_steps: [0]\n",
    "dropout: [0.1]\n",
    "attention_dropout: [0.1]\n",
    "#share_decoder_embeddings: true\n",
    "\n",
    "# Logging\n",
    "log_file: run/logs_no_qoutes\n",
    "\"\"\"\n",
    "config = yaml.safe_load(yaml_config)\n",
    "with open(\"config.yaml\", \"w\") as f:\n",
    "    f.write(yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "appreciated-blackjack",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onmt.utils.parse import ArgumentParser\n",
    "parser = ArgumentParser(description='build_vocab.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prescribed-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onmt.opts import dynamic_prepare_opts\n",
    "dynamic_prepare_opts(parser, build_vocab_only=True) #build_vocab_only=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "romance-dictionary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: E://Data/hu-nmt/combined-en-hu/hunglish2-tiny-train.en\n",
      "  input_format: \n",
      "  model_prefix: sp_models/bpe_en.model\n",
      "  model_type: BPE\n",
      "  vocab_size: 128\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  â�‡ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: E://Data/hu-nmt/combined-en-hu/hunglish2-tiny-train.en\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 128 sentences\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=5374\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=65\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 128 sentences.\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 128\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 611\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=118 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=32 size=20 all=845 active=780 piece=â–�f\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=18 size=40 all=1056 active=991 piece=ad\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12 size=60 all=1170 active=1105 piece=ce\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: sp_models/bpe_en.model.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: sp_models/bpe_en.model.vocab\n"
     ]
    }
   ],
   "source": [
    "data_path = config['data']['hunglish']['path_src']\n",
    "vocab_size = config['src_vocab_size']\n",
    "subword_model = config['src_subword_model']\n",
    "\n",
    "!python utils/spm_train.py -d $data_path -p $subword_model --vocab-size $vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chicken-birthday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1 file(s) moved.\n",
      "        1 file(s) moved.\n"
     ]
    }
   ],
   "source": [
    "!move sp_models\\bpe_en.model.model sp_models\\bpe_en.model\n",
    "!move sp_models\\bpe_en.model.vocab sp_models\\bpe_en.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "valid-colombia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: E://Data/hu-nmt/combined-en-hu/hunglish2-tiny-train.hu\n",
      "  input_format: \n",
      "  model_prefix: sp_models/bpe_hu.model\n",
      "  model_type: BPE\n",
      "  vocab_size: 128\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  â�‡ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: E://Data/hu-nmt/combined-en-hu/hunglish2-tiny-train.hu\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 128 sentences\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=5415\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=73\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 128 sentences.\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 128\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 597\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=94 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=28 size=20 all=1092 active=1019 piece=at\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=17 size=40 all=1365 active=1292 piece=Ã¡l\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: sp_models/bpe_hu.model.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: sp_models/bpe_hu.model.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1 file(s) moved.\n",
      "        1 file(s) moved.\n"
     ]
    }
   ],
   "source": [
    "data_path = config['data']['hunglish']['path_tgt']\n",
    "vocab_size = config['tgt_vocab_size']\n",
    "subword_model = config['tgt_subword_model']\n",
    "\n",
    "!python utils/spm_train.py -d $data_path -p $subword_model --vocab-size $vocab_size\n",
    "!move sp_models\\bpe_hu.model.model sp_models\\bpe_hu.model\n",
    "!move sp_models\\bpe_hu.model.vocab sp_models\\bpe_hu.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "white-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_args = ([\"-config\", \"config.yaml\", \"-n_sample\", \"-1\"])\n",
    "opts, unknown = parser.parse_known_args(base_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unknown-pharmacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(config='config.yaml', data=\"{'hunglish': {'path_src': 'E://Data/hu-nmt/combined-en-hu/hunglish2-tiny-train.en', 'path_tgt': 'E://Data/hu-nmt/combined-en-hu/hunglish2-tiny-train.hu', 'transforms': ['sentencepiece'], 'weight': 1}, 'valid': {'path_src': 'E://Data/hu-nmt/combined-en-hu/hunglish2-tiny-valid.en', 'path_tgt': 'E://Data/hu-nmt/combined-en-hu/hunglish2-tiny-valid.hu', 'transforms': ['sentencepiece']}}\", dump_samples=False, insert_ratio=0.0, mask_length='subword', mask_ratio=0.0, n_sample=-1, num_threads=1, overwrite=False, permute_sent_ratio=0.0, poisson_lambda=3.0, random_ratio=0.0, replace_length=-1, rotate_ratio=0.0, save_config=None, save_data='run/samples', seed=1234, share_vocab=False, skip_empty_level='warning', src_onmttok_kwargs=\"{'mode': 'none'}\", src_seq_length=16, src_subword_alpha=0.0, src_subword_model='sp_models/bpe_en.model', src_subword_nbest=1, src_subword_type='none', src_subword_vocab='', src_vocab='vocabs/vocab.en', src_vocab_threshold=0, switchout_temperature=1.0, tgt_onmttok_kwargs=\"{'mode': 'none'}\", tgt_seq_length=16, tgt_subword_alpha=0.0, tgt_subword_model='sp_models/bpe_hu.model', tgt_subword_nbest=1, tgt_subword_type='none', tgt_subword_vocab='', tgt_vocab='vocabs/vocab.hu', tgt_vocab_threshold=0, tokendrop_temperature=1.0, tokenmask_temperature=1.0, transforms=[], vocab_sample_queue_size=20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "joined-responsibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-04-06 10:50:45,110 INFO] Parsed 2 corpora from -data.\n",
      "[2021-04-06 10:50:45,118 INFO] Counter vocab from -1 samples.\n",
      "[2021-04-06 10:50:45,120 INFO] n_sample=-1: Build vocab on full datasets.\n",
      "[2021-04-06 10:50:46,537 INFO] Counters src:124\n",
      "[2021-04-06 10:50:46,538 INFO] Counters tgt:124\n"
     ]
    }
   ],
   "source": [
    "from onmt.bin.build_vocab import build_vocab_main\n",
    "build_vocab_main(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "settled-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_path = opts.src_vocab\n",
    "tgt_vocab_path = opts.tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "informative-composer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-04-06 10:53:18,120 INFO] Loading src vocabulary from vocabs/vocab.en\n",
      "[2021-04-06 10:53:18,122 INFO] Loaded src vocab has 124 tokens.\n",
      "[2021-04-06 10:53:18,123 INFO] Loading tgt vocabulary from vocabs/vocab.hu\n",
      "[2021-04-06 10:53:18,125 INFO] Loaded tgt vocab has 124 tokens.\n"
     ]
    }
   ],
   "source": [
    "counters = defaultdict(Counter)\n",
    "# load source vocab\n",
    "_src_vocab, _src_vocab_size = _load_vocab(\n",
    "    src_vocab_path,\n",
    "    'src',\n",
    "    counters)\n",
    "# load target vocab\n",
    "_tgt_vocab, _tgt_vocab_size = _load_vocab(\n",
    "    tgt_vocab_path,\n",
    "    'tgt',\n",
    "    counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "impressed-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize fields\n",
    "src_nfeats, tgt_nfeats = 0, 0 # do not support word features for now\n",
    "fields = get_fields(\n",
    "    'text', src_nfeats, tgt_nfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lasting-controversy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': <onmt.inputters.text_dataset.TextMultiField at 0x23c7487ae50>,\n",
       " 'tgt': <onmt.inputters.text_dataset.TextMultiField at 0x23c7487a190>,\n",
       " 'indices': <torchtext.data.field.Field at 0x23c7487a2b0>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "brutal-interim",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-04-06 10:56:05,239 INFO]  * tgt vocab size: 128.\n",
      "[2021-04-06 10:56:05,241 INFO]  * src vocab size: 126.\n"
     ]
    }
   ],
   "source": [
    "# build fields vocab\n",
    "share_vocab = False\n",
    "vocab_size_multiple = 1\n",
    "src_vocab_size = config['src_vocab_size']\n",
    "tgt_vocab_size = config['tgt_vocab_size']\n",
    "src_words_min_frequency = 1\n",
    "tgt_words_min_frequency = 1\n",
    "vocab_fields = _build_fields_vocab(\n",
    "    fields, counters, 'text', share_vocab,\n",
    "    vocab_size_multiple,\n",
    "    src_vocab_size, src_words_min_frequency,\n",
    "    tgt_vocab_size, tgt_words_min_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "pharmaceutical-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text_field = vocab_fields[\"src\"].base_field\n",
    "src_vocab = src_text_field.vocab\n",
    "src_padding = src_vocab.stoi[src_text_field.pad_token]\n",
    "\n",
    "tgt_text_field = vocab_fields['tgt'].base_field\n",
    "tgt_vocab = tgt_text_field.vocab\n",
    "tgt_padding = tgt_vocab.stoi[tgt_text_field.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "prime-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 100\n",
    "rnn_size = 500\n",
    "# Specify the core model.\n",
    "\n",
    "encoder_embeddings = onmt.modules.Embeddings(emb_size, len(src_vocab),\n",
    "                                             word_padding_idx=src_padding)\n",
    "\n",
    "encoder = onmt.encoders.RNNEncoder(hidden_size=rnn_size, num_layers=1,\n",
    "                                   rnn_type=\"LSTM\", bidirectional=True,\n",
    "                                   embeddings=encoder_embeddings)\n",
    "\n",
    "decoder_embeddings = onmt.modules.Embeddings(emb_size, len(tgt_vocab),\n",
    "                                             word_padding_idx=tgt_padding)\n",
    "decoder = onmt.decoders.decoder.InputFeedRNNDecoder(\n",
    "    hidden_size=rnn_size, num_layers=1, bidirectional_encoder=True, \n",
    "    rnn_type=\"LSTM\", embeddings=decoder_embeddings)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = onmt.models.model.NMTModel(encoder, decoder)\n",
    "model.to(device)\n",
    "\n",
    "# Specify the tgt word generator and loss computation module\n",
    "model.generator = nn.Sequential(\n",
    "    nn.Linear(rnn_size, len(tgt_vocab)),\n",
    "    nn.LogSoftmax(dim=-1)).to(device)\n",
    "\n",
    "loss = onmt.utils.loss.NMTLossCompute(\n",
    "    criterion=nn.NLLLoss(ignore_index=tgt_padding, reduction=\"sum\"),\n",
    "    generator=model.generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "figured-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1\n",
    "torch_optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "optim = onmt.utils.optimizers.Optimizer(\n",
    "    torch_optimizer, learning_rate=lr, max_grad_norm=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "front-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train = opts.data['hunglish']['path_src']\n",
    "tgt_train =opts.data['hunglish']['path_tgt']\n",
    "src_val = opts.data['valid']['path_src']\n",
    "tgt_val = opts.data['valid']['path_tgt']\n",
    "\n",
    "# build the ParallelCorpus\n",
    "corpus = ParallelCorpus(\"corpus\", src_train, tgt_train)\n",
    "valid = ParallelCorpus(\"valid\", src_val, tgt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "simplified-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece\n",
    "\n",
    "# build the training iterator\n",
    "train_iter = DynamicDatasetIter(\n",
    "    corpora={\"corpus\": corpus},\n",
    "    corpora_info={\"corpus\": {\"weight\": 1}},\n",
    "    transforms={sentencepiece},\n",
    "    fields=vocab_fields,\n",
    "    is_train=True,\n",
    "    batch_type=\"sents\",\n",
    "    batch_size=8,\n",
    "    batch_size_multiple=1,\n",
    "    data_type=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "agricultural-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the iteration happens on GPU 0 (-1 for CPU, N for GPU N)\n",
    "train_iter = iter(IterOnDevice(train_iter, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "latin-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the validation iterator\n",
    "valid_iter = DynamicDatasetIter(\n",
    "    corpora={\"valid\": valid},\n",
    "    corpora_info={\"valid\": {\"weight\": 1}},\n",
    "    transforms={},\n",
    "    fields=vocab_fields,\n",
    "    is_train=False,\n",
    "    batch_type=\"tokens\",\n",
    "    batch_size=8*16,\n",
    "    batch_size_multiple=1,\n",
    "    data_type=\"text\")\n",
    "\n",
    "valid_iter = IterOnDevice(valid_iter, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "electronic-assembly",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-04-06 11:04:03,829 INFO] Start training loop and validate every 5 steps...\n",
      "[2021-04-06 11:04:03,873 INFO] Step 23/   30; acc:  10.00; ppl: 218.68; xent: 5.39; lr: 1.00000; 1904/1904 tok/s;      0 sec\n",
      "[2021-04-06 11:04:03,904 INFO] Step 24/   30; acc:  50.00; ppl: 89.15; xent: 4.49; lr: 1.00000; 2482/1930 tok/s;      0 sec\n",
      "[2021-04-06 11:04:03,946 INFO] Step 25/   30; acc:  81.82; ppl:  2.47; xent: 0.90; lr: 1.00000; 2800/2200 tok/s;      0 sec\n",
      "[2021-04-06 11:04:03,949 INFO] Loading ParallelCorpus(E://Data/hu-nmt/combined-en-hu/hunglish2-tiny-valid.en, E://Data/hu-nmt/combined-en-hu/hunglish2-tiny-valid.hu, align=None)...\n",
      "[2021-04-06 11:04:04,248 INFO] Validation perplexity: 5.25635\n",
      "[2021-04-06 11:04:04,249 INFO] Validation accuracy: 78.0462\n",
      "[2021-04-06 11:04:04,287 INFO] Step 26/   30; acc:  76.25; ppl:  6.90; xent: 1.93; lr: 1.00000; 260/236 tok/s;      0 sec\n",
      "[2021-04-06 11:04:04,331 INFO] Step 27/   30; acc:  18.18; ppl: 19.91; xent: 2.99; lr: 1.00000; 1523/2095 tok/s;      0 sec\n",
      "[2021-04-06 11:04:04,348 INFO] Step 28/   30; acc:  50.00; ppl:  7.14; xent: 1.97; lr: 1.00000; 533/1066 tok/s;      1 sec\n",
      "[2021-04-06 11:04:04,366 INFO] Step 29/   30; acc:  50.00; ppl:  3.60; xent: 1.28; lr: 1.00000; 500/999 tok/s;      1 sec\n",
      "[2021-04-06 11:04:04,388 INFO] Step 30/   30; acc:  66.67; ppl:  2.19; xent: 0.78; lr: 1.00000; 1999/1199 tok/s;      1 sec\n",
      "[2021-04-06 11:04:04,390 INFO] Loading ParallelCorpus(E://Data/hu-nmt/combined-en-hu/hunglish2-tiny-valid.en, E://Data/hu-nmt/combined-en-hu/hunglish2-tiny-valid.hu, align=None)...\n",
      "[2021-04-06 11:04:04,710 INFO] Validation perplexity: 548.387\n",
      "[2021-04-06 11:04:04,711 INFO] Validation accuracy: 28.6765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<onmt.utils.statistics.Statistics at 0x23c2afdbca0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_manager = onmt.utils.ReportMgr(\n",
    "    report_every=1, start_time=None, tensorboard_writer=None)\n",
    "\n",
    "trainer = onmt.Trainer(model=model,\n",
    "                       train_loss=loss,\n",
    "                       valid_loss=loss,\n",
    "                       optim=optim,\n",
    "                       report_manager=report_manager,\n",
    "                       dropout=[0.1])\n",
    "\n",
    "trainer.train(train_iter=train_iter,\n",
    "              train_steps=30,\n",
    "              valid_iter=valid_iter,\n",
    "              valid_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "systematic-cambodia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E://Data/hu-nmt/combined-en-hu/hunglish2-tiny-valid.en'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts.data['valid']['path_src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "terminal-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "??onmt.encoders.RNNEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "planned-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "base = AutoModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "exposed-costa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-04-06 16:42:54,232 INFO] Lock 2464992055056 acquired on C:\\Users\\gbenc/.cache\\huggingface\\transformers\\6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef17a6c3f1c2442cb13b26e21b5b23a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-04-06 16:42:54,970 INFO] Lock 2464992055056 released on C:\\Users\\gbenc/.cache\\huggingface\\transformers\\6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791.lock\n",
      "[2021-04-06 16:42:55,351 INFO] Lock 2464992054240 acquired on C:\\Users\\gbenc/.cache\\huggingface\\transformers\\226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a439e1eac3904654a5c317cc3d52529a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-04-06 16:42:56,409 INFO] Lock 2464992054240 released on C:\\Users\\gbenc/.cache\\huggingface\\transformers\\226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6.lock\n",
      "[2021-04-06 16:42:57,568 INFO] Lock 2459986215696 acquired on C:\\Users\\gbenc/.cache\\huggingface\\transformers\\ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d64313dd4047a2bf7669138a44711c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-04-06 16:42:57,987 INFO] Lock 2459986215696 released on C:\\Users\\gbenc/.cache\\huggingface\\transformers\\ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f.lock\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "speaking-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "??tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ecological-fruit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer\\\\vocab.txt',)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_vocabulary(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "loving-active",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/vocab.txt',\n",
       " 'tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "recreational-billion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Catholic': 2336,\n",
       " 'Dodge': 14205,\n",
       " 'refugees': 8940,\n",
       " 'Yi': 14141,\n",
       " 'Lands': 17854,\n",
       " 'communion': 27782,\n",
       " 'spoken': 4606,\n",
       " 'Finland': 5776,\n",
       " 'mystery': 8069,\n",
       " 'competitive': 6591,\n",
       " 'association': 3852,\n",
       " 'Fellowship': 9508,\n",
       " 'souls': 11191,\n",
       " 'Source': 5313,\n",
       " 'XV': 16925,\n",
       " 'elder': 8110,\n",
       " 'adulthood': 22777,\n",
       " 'stamped': 23245,\n",
       " 'rescued': 10043,\n",
       " 'Levy': 16809,\n",
       " 'Enforcement': 22990,\n",
       " 'Lublin': 20588,\n",
       " '##ogel': 27732,\n",
       " 'succeed': 9381,\n",
       " 'Confederation': 13052,\n",
       " 'omitted': 17852,\n",
       " 'nut': 22664,\n",
       " 'nutrients': 22667,\n",
       " 'launching': 12611,\n",
       " 'vintage': 17787,\n",
       " 'box': 2884,\n",
       " 'flicked': 12988,\n",
       " 'rejoined': 14944,\n",
       " '##ruba': 24325,\n",
       " '##ía': 7171,\n",
       " 'Mix': 7347,\n",
       " 'salaries': 23343,\n",
       " 'See': 3969,\n",
       " 'ordering': 13649,\n",
       " 'shrug': 13786,\n",
       " 'wines': 16728,\n",
       " 'uk': 26006,\n",
       " '##ector': 20302,\n",
       " 'Chester': 8459,\n",
       " '##leaf': 21407,\n",
       " 'accordion': 22827,\n",
       " 'Alive': 15907,\n",
       " 'examinations': 17865,\n",
       " 'ể': 747,\n",
       " 'grass': 5282,\n",
       " '்': 682,\n",
       " 'desperately': 9600,\n",
       " 'drainage': 12779,\n",
       " '90s': 18476,\n",
       " 'Gorge': 26496,\n",
       " 'gaze': 2806,\n",
       " 'comedian': 10475,\n",
       " '##國': 28898,\n",
       " 'upcoming': 8851,\n",
       " 'waist': 5111,\n",
       " 'communications': 6678,\n",
       " '##smos': 18818,\n",
       " '##ţ': 28248,\n",
       " 'overhead': 9008,\n",
       " 'keep': 1712,\n",
       " 'Nana': 21639,\n",
       " 'Calder': 21056,\n",
       " '##rab': 17952,\n",
       " 'Cambodian': 27463,\n",
       " 'recording': 2730,\n",
       " 'Russia': 2733,\n",
       " 'Sue': 11728,\n",
       " 'Spokane': 24193,\n",
       " 'Boyer': 26170,\n",
       " 'Origins': 27358,\n",
       " 'engine': 2395,\n",
       " 'helmet': 10815,\n",
       " 'Carolyn': 16493,\n",
       " '##mizing': 25596,\n",
       " 'Indies': 9195,\n",
       " 'photographer': 8152,\n",
       " 'Preparatory': 26329,\n",
       " 'bowl': 7329,\n",
       " 'NRHP': 24894,\n",
       " 'core': 4160,\n",
       " 'made': 1189,\n",
       " 'refuse': 10250,\n",
       " 'copyright': 11409,\n",
       " 'market': 2319,\n",
       " 'Dimitri': 16887,\n",
       " 'sprawled': 23671,\n",
       " 'Pakistan': 3658,\n",
       " '##volved': 19628,\n",
       " 'emerald': 23970,\n",
       " 'earl': 26593,\n",
       " '√': 854,\n",
       " '##ira': 5132,\n",
       " '##one': 4798,\n",
       " 'Kid': 9847,\n",
       " 'boiling': 17913,\n",
       " '##ogies': 22339,\n",
       " 'synthesizers': 26000,\n",
       " '##37': 26303,\n",
       " 'act': 2496,\n",
       " 'Blacks': 21861,\n",
       " 'Layne': 24926,\n",
       " 'meaningful': 17119,\n",
       " 'Souls': 22571,\n",
       " 'Vampire': 13453,\n",
       " '##rro': 13656,\n",
       " 'migrated': 13793,\n",
       " 'constructions': 26674,\n",
       " 'tapped': 10316,\n",
       " 'Mai': 17551,\n",
       " 'centre': 2642,\n",
       " 'Cardinal': 8538,\n",
       " 'Balkan': 18903,\n",
       " 'react': 10573,\n",
       " 'technically': 12444,\n",
       " 'Home': 3341,\n",
       " 'snack': 23029,\n",
       " 'greater': 3407,\n",
       " 'fumble': 21577,\n",
       " '##shū': 27365,\n",
       " 'Braun': 24317,\n",
       " '[UNK]': 100,\n",
       " 'investigate': 8242,\n",
       " 'legends': 14218,\n",
       " '##ele': 11194,\n",
       " 'shipped': 12212,\n",
       " '##kot': 27952,\n",
       " 'batsman': 14526,\n",
       " '##von': 19988,\n",
       " '##UN': 27370,\n",
       " 'knives': 14726,\n",
       " 'nail': 16255,\n",
       " 'screw': 13084,\n",
       " 'artillery': 6201,\n",
       " '##fires': 20259,\n",
       " 'talked': 5029,\n",
       " 'stressed': 13713,\n",
       " '1859': 7707,\n",
       " '##met': 11006,\n",
       " '##bius': 19071,\n",
       " 'Restaurant': 17925,\n",
       " 'tied': 4353,\n",
       " '##yana': 23732,\n",
       " 'Rainbow': 13188,\n",
       " 'Bartlett': 21912,\n",
       " 'Spy': 19212,\n",
       " 'trimmed': 24509,\n",
       " 'Brick': 19217,\n",
       " 'doi': 9465,\n",
       " 'ratings': 8532,\n",
       " 'psychologist': 16979,\n",
       " 'Estadio': 19116,\n",
       " 'Tokugawa': 24424,\n",
       " 'provincial': 5586,\n",
       " 'ca': 11019,\n",
       " 'Catholicism': 17164,\n",
       " 'discovered': 2751,\n",
       " 'Jury': 15169,\n",
       " 'Profile': 26890,\n",
       " 'Billboard': 4192,\n",
       " 'Glenn': 9172,\n",
       " 'jerk': 12935,\n",
       " 'II': 1563,\n",
       " 'predominantly': 8941,\n",
       " 'Bond': 8211,\n",
       " 'kg': 4023,\n",
       " 'napkin': 22783,\n",
       " '##OM': 13041,\n",
       " 'Certain': 16482,\n",
       " 'Lower': 5738,\n",
       " 'executed': 5858,\n",
       " 'younger': 3247,\n",
       " 'apparent': 6281,\n",
       " 'Friedrich': 8523,\n",
       " '##een': 9561,\n",
       " 'interchange': 9629,\n",
       " 'whispering': 15181,\n",
       " 'cyclone': 15202,\n",
       " 'textiles': 23469,\n",
       " 'working': 1684,\n",
       " 'Incorporated': 25800,\n",
       " 'Nuevo': 26118,\n",
       " '2014': 1387,\n",
       " '##chen': 10415,\n",
       " 'Linux': 11735,\n",
       " '##gles': 15657,\n",
       " '##safe': 27789,\n",
       " '[unused2]': 2,\n",
       " '##ffed': 9845,\n",
       " 'Yeah': 2814,\n",
       " 'widely': 3409,\n",
       " 'differentiate': 23159,\n",
       " 'morality': 19613,\n",
       " 'meant': 2318,\n",
       " 'Toro': 27470,\n",
       " 'cartridge': 16542,\n",
       " '220': 10423,\n",
       " 'sufficiently': 13230,\n",
       " '##cine': 15459,\n",
       " '##ters': 5759,\n",
       " 'Welsh': 5447,\n",
       " 'Leaders': 20880,\n",
       " 'Shaw': 7802,\n",
       " 'activation': 14915,\n",
       " '##ju': 9380,\n",
       " 'partner': 3547,\n",
       " 'aisle': 12899,\n",
       " 'Studies': 3829,\n",
       " 'specialized': 7623,\n",
       " '##bis': 14866,\n",
       " 'hillside': 25068,\n",
       " '##itrus': 22220,\n",
       " 'Kiel': 23144,\n",
       " '65': 2625,\n",
       " '##awed': 21449,\n",
       " 'enterprise': 10614,\n",
       " '##fast': 27704,\n",
       " '##teca': 27560,\n",
       " '##bu': 7925,\n",
       " 'But': 1252,\n",
       " 'supervision': 10955,\n",
       " 'Ellington': 23884,\n",
       " 'damages': 12947,\n",
       " 'Nazi': 5755,\n",
       " 'Wine': 15054,\n",
       " 'Excellent': 25764,\n",
       " 'Irene': 13524,\n",
       " 'disgusted': 20524,\n",
       " 'successor': 5714,\n",
       " 'closed': 1804,\n",
       " 'Achievement': 10295,\n",
       " '##inen': 19865,\n",
       " '##kim': 18504,\n",
       " 'designing': 13795,\n",
       " 'integrating': 26975,\n",
       " 'gently': 4588,\n",
       " '##pse': 17783,\n",
       " 'focus': 2817,\n",
       " '16th': 5050,\n",
       " 'ownership': 5582,\n",
       " 'bench': 6757,\n",
       " '##writer': 13814,\n",
       " 'pier': 16331,\n",
       " '##adores': 27217,\n",
       " '##lated': 6951,\n",
       " 'Micro': 27730,\n",
       " '##bers': 11697,\n",
       " 'destroyed': 3072,\n",
       " 'EMI': 13435,\n",
       " 'rated': 6317,\n",
       " 'insides': 20751,\n",
       " 'Comprehensive': 25187,\n",
       " 'Ion': 19447,\n",
       " 'sinister': 20975,\n",
       " 'Constance': 16651,\n",
       " 'award': 2574,\n",
       " 'Lillian': 21044,\n",
       " 'Doyle': 11296,\n",
       " 'Walking': 13181,\n",
       " 'Sara': 6936,\n",
       " 'ji': 23220,\n",
       " '1886': 6332,\n",
       " 'streamed': 20273,\n",
       " '辶': 1073,\n",
       " '##rn': 4558,\n",
       " 'Lamb': 16978,\n",
       " 'grinned': 6375,\n",
       " 'Nuremberg': 21072,\n",
       " 'Miles': 7726,\n",
       " '##nt': 2227,\n",
       " 'Jean': 2893,\n",
       " '1924': 4002,\n",
       " 'Cheung': 26475,\n",
       " 'sixteenth': 16704,\n",
       " 'torture': 8831,\n",
       " 'Anatomy': 26512,\n",
       " '##;': 28141,\n",
       " 'assessment': 8670,\n",
       " '##̃': 28311,\n",
       " '##iz': 9368,\n",
       " '##shes': 26816,\n",
       " '116': 13096,\n",
       " 'attention': 2209,\n",
       " 'Supply': 17153,\n",
       " 'Denmark': 5140,\n",
       " 'supposedly': 11178,\n",
       " 'capped': 14510,\n",
       " 'hearth': 27965,\n",
       " '700': 5689,\n",
       " '##N': 2249,\n",
       " 'defines': 12028,\n",
       " '##per': 3365,\n",
       " '##illo': 12284,\n",
       " '##boe': 21968,\n",
       " 'motion': 4018,\n",
       " 'biblical': 16239,\n",
       " 'aux': 24544,\n",
       " 'sustaining': 26081,\n",
       " 'bank': 3085,\n",
       " 'loaned': 13607,\n",
       " 'revolt': 11733,\n",
       " 'nurses': 13318,\n",
       " 'Republican': 3215,\n",
       " 'Flat': 20451,\n",
       " 'authors': 5752,\n",
       " '##lers': 10369,\n",
       " 'formulation': 22661,\n",
       " 'Barclay': 26560,\n",
       " 'candidacy': 18364,\n",
       " 'Done': 24243,\n",
       " 'tools': 5537,\n",
       " 'signs': 5300,\n",
       " 'Canal': 6327,\n",
       " '##back': 4197,\n",
       " 'sparked': 14514,\n",
       " 'Brain': 15240,\n",
       " '##lift': 12215,\n",
       " 'jumps': 15457,\n",
       " 'diplomat': 11608,\n",
       " 'む': 920,\n",
       " '53': 4389,\n",
       " '##dos': 12847,\n",
       " 'fortifications': 15542,\n",
       " 'Ba': 18757,\n",
       " 'cherry': 20170,\n",
       " 'Lombard': 26416,\n",
       " 'centuries': 3944,\n",
       " 'bullshit': 17480,\n",
       " 'write': 3593,\n",
       " 'Richard': 2055,\n",
       " 'ease': 7166,\n",
       " 'pathway': 13548,\n",
       " 'Louis': 2535,\n",
       " '¹': 219,\n",
       " 'palm': 5824,\n",
       " '122': 13381,\n",
       " '##iera': 25579,\n",
       " 'Heritage': 5560,\n",
       " 'specialist': 9131,\n",
       " 'Rothschild': 21457,\n",
       " '##sik': 24577,\n",
       " 'Public': 2710,\n",
       " 'voting': 6612,\n",
       " 'Banner': 18985,\n",
       " 'S': 156,\n",
       " 'This': 1188,\n",
       " '46': 3993,\n",
       " '##mons': 17199,\n",
       " 'Pack': 14667,\n",
       " 'Max': 3405,\n",
       " 'informs': 16788,\n",
       " 'Jesus': 3766,\n",
       " 'recruit': 14240,\n",
       " 'modern': 2030,\n",
       " 'ν': 430,\n",
       " 'Six': 4995,\n",
       " 'slept': 7362,\n",
       " 'philosophical': 11388,\n",
       " 'Whitman': 23459,\n",
       " '##ᵤ': 28624,\n",
       " 'mused': 24807,\n",
       " '##board': 4015,\n",
       " 'costa': 20658,\n",
       " 'prolific': 13127,\n",
       " 'Ren': 16513,\n",
       " 'safer': 15033,\n",
       " '##liner': 25167,\n",
       " 'use': 1329,\n",
       " 'Shell': 15769,\n",
       " 'Israel': 3103,\n",
       " 'classrooms': 13038,\n",
       " 'WWF': 17860,\n",
       " '##iana': 12268,\n",
       " 'Piece': 27916,\n",
       " 'Lincoln': 4617,\n",
       " '##₅': 28712,\n",
       " 'whilst': 5994,\n",
       " 'efficient': 7856,\n",
       " 'デ': 952,\n",
       " 'Inn': 9859,\n",
       " 'respective': 7514,\n",
       " 'President': 1697,\n",
       " '27th': 15757,\n",
       " 'Greater': 6752,\n",
       " '1951': 3280,\n",
       " 'Matthew': 4754,\n",
       " '7': 128,\n",
       " '##da': 1810,\n",
       " 'Janeiro': 11502,\n",
       " 'Text': 18430,\n",
       " '##bound': 8346,\n",
       " '##cial': 12562,\n",
       " '##chy': 8992,\n",
       " 'source': 2674,\n",
       " 'threads': 18636,\n",
       " 'Bel': 26744,\n",
       " 'enforce': 17542,\n",
       " '##achi': 19226,\n",
       " '##ugs': 22188,\n",
       " 'reopened': 11996,\n",
       " '##nose': 22583,\n",
       " 'Key': 7443,\n",
       " 'cattle': 6937,\n",
       " '##м': 28401,\n",
       " 'drafted': 7071,\n",
       " 'Beaufort': 26427,\n",
       " 'physicians': 15230,\n",
       " 'imports': 20171,\n",
       " 'RB': 24718,\n",
       " '##té': 14608,\n",
       " 'Shirley': 11281,\n",
       " 'limp': 15905,\n",
       " 'Cycle': 19991,\n",
       " 'Germain': 20920,\n",
       " 'cover': 2267,\n",
       " 'Valencia': 13697,\n",
       " 'Charlton': 19155,\n",
       " 'Ḥ': 727,\n",
       " '##rrow': 8674,\n",
       " 'routing': 19044,\n",
       " 'ṇ': 734,\n",
       " 'Suit': 27331,\n",
       " 'tasted': 12876,\n",
       " '1955': 3115,\n",
       " 'risk': 3187,\n",
       " '##eu': 14272,\n",
       " 'ceilings': 23679,\n",
       " '##san': 9995,\n",
       " '[': 164,\n",
       " '##yuan': 19835,\n",
       " 'Henrietta': 22807,\n",
       " 'piano': 3267,\n",
       " 'Trade': 5820,\n",
       " 'fullback': 25540,\n",
       " 'Average': 18098,\n",
       " '##ential': 15544,\n",
       " 'Texans': 25904,\n",
       " 'volumes': 6357,\n",
       " 'ₑ': 825,\n",
       " 'Kitchen': 18988,\n",
       " '##idian': 27967,\n",
       " '##ᵀ': 28611,\n",
       " 'Andreas': 12651,\n",
       " 'Archives': 12422,\n",
       " 'interact': 12254,\n",
       " 'relate': 15123,\n",
       " '##tric': 11048,\n",
       " 'villain': 14081,\n",
       " 'gentle': 6892,\n",
       " 'boards': 8190,\n",
       " 'Wolf': 6499,\n",
       " '##ère': 7436,\n",
       " 'Phelps': 22385,\n",
       " 'hiking': 14249,\n",
       " '##chschule': 27357,\n",
       " 'PDF': 12955,\n",
       " 'slapped': 11176,\n",
       " 'Tank': 12466,\n",
       " 'Imam': 21765,\n",
       " 'Invitational': 26897,\n",
       " 'bulb': 23447,\n",
       " 'Ibn': 14340,\n",
       " 'Leeds': 7383,\n",
       " 'quota': 23690,\n",
       " 'shower': 5946,\n",
       " 'Hodge': 26201,\n",
       " 'unitary': 26217,\n",
       " 'hired': 4327,\n",
       " '##use': 5613,\n",
       " 'Jaime': 15162,\n",
       " 'sunset': 16855,\n",
       " 'monument': 7020,\n",
       " 'beetle': 7924,\n",
       " 'paradise': 23876,\n",
       " 'Spirits': 27761,\n",
       " 'drew': 3583,\n",
       " 'hire': 11562,\n",
       " 'Appearance': 27717,\n",
       " 'Alternative': 13069,\n",
       " '##iring': 14641,\n",
       " '##ậ': 28645,\n",
       " 'Teenage': 25830,\n",
       " 'stock': 4482,\n",
       " 'Kay': 11247,\n",
       " 'batted': 13033,\n",
       " 'Figure': 15982,\n",
       " '##fall': 8877,\n",
       " '29': 1853,\n",
       " 'involved': 2017,\n",
       " 'abroad': 6629,\n",
       " '##ud': 4867,\n",
       " 'Workshop': 16350,\n",
       " '##bation': 19632,\n",
       " 'answer': 2590,\n",
       " 'Wellington': 8017,\n",
       " 'spokesperson': 18391,\n",
       " 'assignments': 15799,\n",
       " '##ars': 7666,\n",
       " '80': 2908,\n",
       " 'experimenting': 27193,\n",
       " '[unused99]': 99,\n",
       " 'ministers': 9813,\n",
       " 'incomplete': 13975,\n",
       " '##chel': 20492,\n",
       " 'Isaac': 7026,\n",
       " 'Turks': 13126,\n",
       " 'Celebrity': 20448,\n",
       " '##GB': 13745,\n",
       " '##hog': 27892,\n",
       " '⟨': 882,\n",
       " '##ophone': 17826,\n",
       " '##ural': 12602,\n",
       " 'Leinster': 16619,\n",
       " 'definitive': 18379,\n",
       " 'orbits': 22508,\n",
       " 'walked': 2045,\n",
       " 'Guam': 17256,\n",
       " 'Negro': 13898,\n",
       " 'shrieked': 24863,\n",
       " 'earth': 4033,\n",
       " 'successors': 20475,\n",
       " '##charged': 23131,\n",
       " '##nger': 11576,\n",
       " 'PBA': 23867,\n",
       " 'result': 1871,\n",
       " 'ů': 330,\n",
       " '##ロ': 28865,\n",
       " 'glow': 8656,\n",
       " 'Bernardo': 23333,\n",
       " 'Jennifer': 7184,\n",
       " '##rg': 10805,\n",
       " '##hy': 7889,\n",
       " 'lease': 10549,\n",
       " 'chancel': 17954,\n",
       " 'API': 20480,\n",
       " 'relaxed': 8000,\n",
       " 'federally': 26129,\n",
       " 'Banking': 20863,\n",
       " 'modules': 15412,\n",
       " 'clothing': 5413,\n",
       " 'Aristotle': 18727,\n",
       " '##irs': 11836,\n",
       " 'install': 17812,\n",
       " 'visibly': 21408,\n",
       " 'botanical': 20764,\n",
       " '##dote': 26595,\n",
       " '##¶': 28171,\n",
       " 'Gil': 13876,\n",
       " 'ť': 326,\n",
       " '##BE': 27211,\n",
       " 'concussion': 27233,\n",
       " 'complain': 19073,\n",
       " '##ety': 20656,\n",
       " 'jackets': 27310,\n",
       " '##sitic': 27702,\n",
       " 'embarked': 11322,\n",
       " 'TD': 15439,\n",
       " 'Nelson': 5232,\n",
       " 'Perez': 20007,\n",
       " 'Celia': 19629,\n",
       " 'Crystal': 9048,\n",
       " '##psis': 17990,\n",
       " 'Martins': 22670,\n",
       " 'diver': 23448,\n",
       " 'wheel': 4829,\n",
       " 'MTV': 8316,\n",
       " 'donors': 20023,\n",
       " 'Net': 20820,\n",
       " 'thoughtfully': 21815,\n",
       " 'injected': 21881,\n",
       " 'Lies': 22550,\n",
       " 'lions': 20050,\n",
       " 'tongues': 24403,\n",
       " 'admiral': 15822,\n",
       " 'disabilities': 16292,\n",
       " 'Norman': 5177,\n",
       " 'Godfrey': 19635,\n",
       " 'Buzz': 22687,\n",
       " 'convoy': 10643,\n",
       " '##iet': 22331,\n",
       " 'Elsa': 26709,\n",
       " 'Watching': 21570,\n",
       " 'proof': 6777,\n",
       " 'peers': 14097,\n",
       " 'm²': 6450,\n",
       " '##ico': 10658,\n",
       " 'variation': 8516,\n",
       " 'Recorded': 13048,\n",
       " 'Dodgers': 13839,\n",
       " 'outcome': 9386,\n",
       " 'urging': 14992,\n",
       " 'engages': 27454,\n",
       " 'archaeological': 8962,\n",
       " '##ou': 6094,\n",
       " 'ö': 268,\n",
       " 'lacks': 14756,\n",
       " 'barge': 24237,\n",
       " '##mail': 14746,\n",
       " 'Units': 21687,\n",
       " '##ptic': 8956,\n",
       " '##ƒ': 28263,\n",
       " 'semester': 14594,\n",
       " 'Movement': 6257,\n",
       " 'Botanical': 24136,\n",
       " '##lumber': 26993,\n",
       " 'Buddy': 11999,\n",
       " '##asa': 18384,\n",
       " 'Maud': 24351,\n",
       " '##：': 28995,\n",
       " 'transferred': 3175,\n",
       " 'Somebody': 18490,\n",
       " 'elections': 3212,\n",
       " 'Herald': 9707,\n",
       " 'cheek': 4310,\n",
       " 'ponytail': 21101,\n",
       " 'dismiss': 21728,\n",
       " 'hopped': 18507,\n",
       " 'Jung': 13279,\n",
       " 'テ': 951,\n",
       " '1997': 1816,\n",
       " 'mechanics': 11556,\n",
       " 'seventh': 5001,\n",
       " 'blushed': 21082,\n",
       " 'cost': 2616,\n",
       " 'subsidiaries': 22423,\n",
       " 'Federation': 4245,\n",
       " 'Tracy': 10435,\n",
       " '##eper': 25136,\n",
       " 'pilot': 3955,\n",
       " '[unused11]': 11,\n",
       " '##ckling': 27102,\n",
       " 'symbolic': 13516,\n",
       " 'und': 5576,\n",
       " 'bassist': 10042,\n",
       " '##chtenstein': 23561,\n",
       " '##wayne': 25920,\n",
       " '##uer': 10232,\n",
       " 'derived': 4408,\n",
       " '##erd': 25081,\n",
       " 'Sophie': 7800,\n",
       " '##our': 6334,\n",
       " 'adds': 9807,\n",
       " 'ushered': 26751,\n",
       " 'Eileen': 22431,\n",
       " 'Eighth': 17561,\n",
       " '##ov': 3292,\n",
       " '##inger': 7728,\n",
       " 'accepted': 3134,\n",
       " 'cloth': 8217,\n",
       " '##ₖ': 28725,\n",
       " '##point': 7587,\n",
       " 'Steps': 27913,\n",
       " 'bother': 8255,\n",
       " 'ICC': 17502,\n",
       " 'Hernández': 22844,\n",
       " 'moving': 2232,\n",
       " '##்': 28585,\n",
       " 'stresses': 26183,\n",
       " 'banged': 25697,\n",
       " '##hara': 10131,\n",
       " 'sped': 18132,\n",
       " 'Ν': 405,\n",
       " 'ok': 21534,\n",
       " 'Herzegovina': 11464,\n",
       " '##TH': 24162,\n",
       " 'Mace': 28013,\n",
       " 'flattened': 17454,\n",
       " '##occo': 26108,\n",
       " 'Reagan': 11546,\n",
       " 'Jacques': 6909,\n",
       " 'Christian': 2131,\n",
       " '##riad': 20144,\n",
       " '¡': 200,\n",
       " '##nco': 23573,\n",
       " 'stolen': 7251,\n",
       " 'bust': 16118,\n",
       " '##halt': 24537,\n",
       " 'Seas': 24281,\n",
       " '##chard': 16657,\n",
       " 'Fargo': 25727,\n",
       " 'Scouting': 22768,\n",
       " 'puts': 8165,\n",
       " '##NS': 12412,\n",
       " 'pavilion': 17152,\n",
       " '##ique': 5484,\n",
       " 'Langley': 20794,\n",
       " 'shadows': 6719,\n",
       " 'Aragon': 19493,\n",
       " 'Seville': 20768,\n",
       " '##ز': 28483,\n",
       " '##ificant': 18772,\n",
       " 'thrust': 7113,\n",
       " 'hitting': 6886,\n",
       " '##cion': 16012,\n",
       " 'Harcourt': 25272,\n",
       " 'Juliet': 14143,\n",
       " '1905': 4761,\n",
       " 'memories': 5628,\n",
       " '##some': 11743,\n",
       " 'Hiroshima': 22091,\n",
       " '##mble': 16465,\n",
       " 'Benton': 20228,\n",
       " 'TV': 1794,\n",
       " 'groundwater': 27218,\n",
       " 'Liang': 18842,\n",
       " 'Captain': 2791,\n",
       " '##vington': 26251,\n",
       " 'wiped': 7960,\n",
       " 'applause': 24670,\n",
       " 'temptation': 23483,\n",
       " 'ᵥ': 723,\n",
       " 'week': 1989,\n",
       " '##vere': 25243,\n",
       " 'ս': 523,\n",
       " 'Trading': 22263,\n",
       " '1913': 4325,\n",
       " '##2': 1477,\n",
       " 'Fremantle': 22231,\n",
       " 'guests': 6065,\n",
       " 'Oscar': 7248,\n",
       " 'Wren': 19088,\n",
       " '##regation': 22998,\n",
       " 'reduced': 3549,\n",
       " 'Hold': 10860,\n",
       " '##dust': 27650,\n",
       " '##sium': 25967,\n",
       " 'detect': 11552,\n",
       " 'Maybe': 2389,\n",
       " '##tor': 2772,\n",
       " '##lify': 22881,\n",
       " 'scenes': 4429,\n",
       " '##ś': 28244,\n",
       " 'discussions': 10508,\n",
       " 'Airfield': 17858,\n",
       " '##acity': 19905,\n",
       " '##ratic': 21961,\n",
       " 'Peña': 25273,\n",
       " 'Hispanic': 6098,\n",
       " 'graduating': 6282,\n",
       " 'Concept': 28103,\n",
       " '##bang': 25903,\n",
       " '##евич': 25525,\n",
       " 'ế': 745,\n",
       " 'NC': 14056,\n",
       " 'signaling': 16085,\n",
       " 'tornadoes': 28035,\n",
       " 'appearances': 3178,\n",
       " 'Sergey': 25252,\n",
       " 'knew': 1450,\n",
       " 'collaborated': 8303,\n",
       " 'Mail': 11508,\n",
       " '##ǔ': 28269,\n",
       " 'Martian': 23194,\n",
       " '##ned': 3540,\n",
       " '##roy': 9166,\n",
       " 'Roe': 26301,\n",
       " '##В': 28368,\n",
       " 'beginning': 2150,\n",
       " 'demolition': 14295,\n",
       " 'Emery': 28019,\n",
       " '##wl': 10481,\n",
       " 'bankrupt': 19004,\n",
       " 'answered': 3845,\n",
       " 'Sabha': 11349,\n",
       " 'ơ': 342,\n",
       " 'Cleveland': 5364,\n",
       " 'bonus': 6992,\n",
       " '##Ż': 28258,\n",
       " '##kowski': 17445,\n",
       " '##sound': 22909,\n",
       " 'Douglas': 4402,\n",
       " 'sketch': 12333,\n",
       " '##ssie': 16869,\n",
       " 'Osborne': 17802,\n",
       " '##aneous': 13064,\n",
       " 'Meadows': 20593,\n",
       " 'conflicting': 21776,\n",
       " 'Album': 6429,\n",
       " 'swap': 24295,\n",
       " '42': 3565,\n",
       " 'madness': 16870,\n",
       " '##MC': 10044,\n",
       " 'formerly': 3147,\n",
       " 'evenly': 19474,\n",
       " '##ạ': 28640,\n",
       " '##throp': 21850,\n",
       " '##mobile': 24052,\n",
       " '##つ': 28803,\n",
       " 'boost': 14112,\n",
       " 'radical': 8276,\n",
       " 'Baltic': 11306,\n",
       " 'landlord': 21406,\n",
       " 'Built': 10521,\n",
       " 'pamphlet': 22461,\n",
       " 'Having': 5823,\n",
       " '##ilo': 24755,\n",
       " '##lette': 21958,\n",
       " 'Geographic': 15472,\n",
       " 'activate': 23162,\n",
       " 'Angry': 24991,\n",
       " 'sources': 3509,\n",
       " 'winners': 5222,\n",
       " '100th': 18026,\n",
       " 'Murdoch': 21736,\n",
       " '##oso': 22354,\n",
       " 'Torres': 13428,\n",
       " 'plane': 4261,\n",
       " '##istice': 20788,\n",
       " 'Lviv': 26681,\n",
       " 'protocol': 11309,\n",
       " 'professors': 14427,\n",
       " 'fluid': 8240,\n",
       " 'Enrique': 16683,\n",
       " 'consecutive': 4776,\n",
       " '##chus': 15548,\n",
       " 'asking': 4107,\n",
       " 'Domain': 20237,\n",
       " 'unconstitutional': 22468,\n",
       " 'ghost': 7483,\n",
       " 'vivid': 16756,\n",
       " 'party': 1710,\n",
       " 'Revival': 8884,\n",
       " 'Stores': 26811,\n",
       " 'taxa': 27522,\n",
       " 'bee': 17775,\n",
       " 'sea': 2343,\n",
       " 'Punjab': 8907,\n",
       " '##cott': 11627,\n",
       " 'hooded': 25076,\n",
       " '##licity': 17432,\n",
       " 'repeatedly': 8038,\n",
       " 'Though': 3473,\n",
       " 'Niall': 24233,\n",
       " 'Sessions': 18003,\n",
       " 'suitable': 6736,\n",
       " 'Gardens': 9035,\n",
       " '##mata': 23436,\n",
       " '##ŭ': 28251,\n",
       " 'teachers': 4952,\n",
       " 'Paisley': 27771,\n",
       " 'driven': 4940,\n",
       " 'Z': 163,\n",
       " 'goal': 2273,\n",
       " 'underneath': 7547,\n",
       " 'canals': 20094,\n",
       " 'publicity': 12623,\n",
       " 'Bears': 10169,\n",
       " '##rst': 9731,\n",
       " 'Jessie': 10911,\n",
       " 'Bring': 15646,\n",
       " 'す': 905,\n",
       " 'cables': 16886,\n",
       " 'ź': 335,\n",
       " '##ist': 1776,\n",
       " 'Ana': 9954,\n",
       " '##pable': 20786,\n",
       " 'Leisure': 26090,\n",
       " 'emerging': 8999,\n",
       " 'novelist': 10050,\n",
       " 'Tipperary': 18521,\n",
       " 'consider': 4615,\n",
       " 'finishing': 4416,\n",
       " 'criticisms': 22316,\n",
       " 'wisdom': 12304,\n",
       " 'responsibility': 4812,\n",
       " 'privacy': 9909,\n",
       " 'transformation': 9047,\n",
       " 'ranch': 10804,\n",
       " '##sure': 14847,\n",
       " 'Hunters': 17998,\n",
       " 'tripped': 23376,\n",
       " '##IG': 23413,\n",
       " '##zhen': 21821,\n",
       " 'State': 1426,\n",
       " 'dispersed': 16478,\n",
       " 'swayed': 22016,\n",
       " 'Trenton': 18338,\n",
       " 'mold': 21532,\n",
       " '##els': 5999,\n",
       " 'embraced': 14891,\n",
       " '##derman': 17951,\n",
       " 'Eliot': 17324,\n",
       " 'scrapped': 15420,\n",
       " 'exploration': 10016,\n",
       " 'WCW': 27189,\n",
       " 'magnitude': 10094,\n",
       " '##zio': 13409,\n",
       " '##rini': 19764,\n",
       " '##lved': 25043,\n",
       " 'Simple': 16896,\n",
       " 'delay': 8513,\n",
       " 'beautiful': 2712,\n",
       " 'carried': 2446,\n",
       " 'delivery': 6779,\n",
       " 'Kilda': 25168,\n",
       " 'leaped': 18021,\n",
       " 'Silent': 15013,\n",
       " 'contribution': 6436,\n",
       " 'beer': 5298,\n",
       " 'Eden': 10253,\n",
       " 'Vince': 12279,\n",
       " 'sold': 1962,\n",
       " 'moved': 1427,\n",
       " 'Enemy': 19269,\n",
       " '##am': 2312,\n",
       " '##pticism': 26265,\n",
       " 'gunshot': 24939,\n",
       " 'disputes': 12530,\n",
       " 'precinct': 26757,\n",
       " 'ho': 16358,\n",
       " 'Organ': 20383,\n",
       " 'Territorial': 14984,\n",
       " 'grouped': 15965,\n",
       " 'Lowe': 14830,\n",
       " 'Nevertheless': 8094,\n",
       " '##gues': 17423,\n",
       " 'reasons': 3672,\n",
       " 'stalked': 16895,\n",
       " '्': 634,\n",
       " 'Economic': 6051,\n",
       " 'finalist': 12039,\n",
       " 'nature': 2731,\n",
       " '##acts': 19523,\n",
       " '##GR': 27617,\n",
       " 'Albany': 10128,\n",
       " 'wave': 4003,\n",
       " '##log': 13791,\n",
       " 'repeating': 16590,\n",
       " 'Softball': 26719,\n",
       " 'depend': 12864,\n",
       " '°': 211,\n",
       " 'tie': 5069,\n",
       " 'parameters': 11934,\n",
       " '1796': 14342,\n",
       " 'ensures': 23613,\n",
       " 'hiring': 15768,\n",
       " 'wrote': 1724,\n",
       " 'some': 1199,\n",
       " 'Navajo': 24954,\n",
       " 'ritual': 9181,\n",
       " 'corresponds': 15497,\n",
       " 'interstate': 20905,\n",
       " 'Lazarus': 26501,\n",
       " 'ი': 700,\n",
       " 'exposing': 15952,\n",
       " '1919': 3688,\n",
       " 'president': 2084,\n",
       " 'biographer': 19382,\n",
       " 'Uruguay': 11752,\n",
       " 'Von': 13610,\n",
       " '##V': 2559,\n",
       " 'Girl': 4537,\n",
       " 'tissue': 7918,\n",
       " 'crowded': 11090,\n",
       " '##Millan': 22266,\n",
       " '##layer': 22938,\n",
       " 'cigarettes': 16595,\n",
       " '##ganized': 24087,\n",
       " 'Knoxville': 21837,\n",
       " 'Research': 2713,\n",
       " '230': 11866,\n",
       " 'Canyon': 10084,\n",
       " '1648': 25020,\n",
       " 'Automatic': 24842,\n",
       " 'Palmer': 8450,\n",
       " '##RS': 8900,\n",
       " 'Band': 4230,\n",
       " 'server': 9770,\n",
       " '##ssion': 16656,\n",
       " 'i': 178,\n",
       " 'collection': 2436,\n",
       " 'horns': 13011,\n",
       " 'Herb': 20780,\n",
       " 'digit': 16521,\n",
       " '1100': 24450,\n",
       " '##Script': 27780,\n",
       " 'relentless': 26011,\n",
       " 'Wild': 5469,\n",
       " 'Harlem': 15557,\n",
       " '501': 17196,\n",
       " 'Guthrie': 21727,\n",
       " '##jack': 19617,\n",
       " 'airline': 8694,\n",
       " 'Ridge': 7082,\n",
       " 'garbage': 14946,\n",
       " 'structures': 4413,\n",
       " 'Victory': 10663,\n",
       " 'forming': 5071,\n",
       " 'betrayed': 12546,\n",
       " '##zar': 8950,\n",
       " 'Chamber': 7018,\n",
       " '##books': 16429,\n",
       " 'sober': 20040,\n",
       " 'tablet': 16048,\n",
       " 'behavior': 4658,\n",
       " 'Kolkata': 13895,\n",
       " ...}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "meaningful-split",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2336, 14141, 102]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Catholic Yi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "descending-birth",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x0a'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-0b4485445d09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sp_models/bpe_en.model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtokenizer_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '\\x0a'."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"sp_models/bpe_en.model\", 'rb') as f:\n",
    "    tokenizer_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "animal-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "processor = spm.SentencePieceProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "beneficial-reasoning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.load(\"sp_models/bpe_en.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "american-yugoslavia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.load_vocabulary(\"sp_models/bpe_en.vocab\", threshold = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "beneficial-wrist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " 'I',\n",
       " '▁',\n",
       " 'a',\n",
       " 'm',\n",
       " '▁',\n",
       " 'r',\n",
       " 'e',\n",
       " 'a',\n",
       " 'l',\n",
       " 'l',\n",
       " 'y',\n",
       " '▁',\n",
       " 's',\n",
       " 'e',\n",
       " 'x',\n",
       " 'y']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.encode_as_pieces(\"I am really sexy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "coupled-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "??processor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
